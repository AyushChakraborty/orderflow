entities are the db schema representation and DTOs are the API contract representation

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

now involving kafka, where we can publish an event to a message broker, and other services
subscribe and react

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

kafka is a distributed, durable event log, where consumers can read at their own pace

docker-compose up -d

to stop and remove the containers defined in the docker compose file
docker-compose down

to just stop them
docker-compose stop

to remove the volumes too
docker-compose down -v

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

wrt kafka

topic: message queue itself
producer: sends messages to the topic
consumer: reads messages from the topic
broker: kafka server, in this case, localhost:9092
zookeeper: coordinates kafka brokers

the kafka process starts up within the docker container, and it needs to bind to port 9092
since it needs to comm with other processes and clients, not necessarily the processes from the
same container

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

to enter a container, do
docker exec -it kafka bash

where kafka is the name of the process here, and it means that we attach a new bash process to
the container, so its like a new process is opened up within the container, hence when this
bash process actually runs, we can interact with the container, this can be seen here:

   [appuser@kafka ~]$ ps
     PID TTY          TIME CMD
     151 pts/0    00:00:00 bash
     167 pts/0    00:00:00 ps
the bash is the process just started in the container, and the way we are able to access it from
the host is the -it flag, where -i means interactive, keeping the STDIN open, and -t means open
a pseudo-TTY session. Docker sets up pipes between the host terminal and the stdin/stdout of the
bash process

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

orderService here is a producer microservice

within this, config class configures the kafka infra, which in this case are the different topics,
and the event producer class actually sends the events

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

while setting up the config for kafka, we look at partitions and replicas

also a single topic is set up to avoid risk of cross topic order issues

the events get sent to 1 of n partitions based on the key's hash. In this case, the key is the
id of the event. So events with the same event id always go to the same partition for that
topic. Hence, the order is maintained

replicas ensure that the events data themselves present in multiple partitions of topics
are present as copies in many brokers (basically nodes in distributed systems here are called
brokers)

@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@@

if a class only has one constructor, spring automatically uses it for dependency injection

